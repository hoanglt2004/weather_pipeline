# ğŸŒ¦ï¸ Weather Data Pipeline  
### ETL Pipeline with Airflow â€¢ PostgreSQL â€¢ Docker â€¢ Metabase

This project is a fully automated **Data Engineering pipeline** that collects real-time weather data from the OpenWeather API, processes it using **Apache Airflow**, stores it in a **PostgreSQL Data Warehouse**, and visualizes insights using **Metabase** dashboards.

The system is built with a production-style architecture and includes:  
**data lake â†’ ETL â†’ DWH â†’ BI â†’ monitoring (Slack + Email)**

---

# ğŸš€ 1. Project Overview

This pipeline processes weather data through the following stages:

### ğŸ”¹ **Extract**
- Fetch weather data from OpenWeather API for multiple cities.
- Store raw JSON into a structured **Data Lake**:
  ```
  data/raw/<city>/<yyyy-mm-dd>/<hour>/weather_*.json
  ```

### ğŸ”¹ **Transform**
- Parse & clean JSON files.
- Feature engineering:
  - `feels_like_diff`
  - `temp_category`
  - `weather_category`
  - `wind_level`
  - `is_rain`
- Output cleaned **Parquet** files.

### ğŸ”¹ **Load**
- Insert transformed data into a **PostgreSQL Data Warehouse** (Star Schema).

### ğŸ”¹ **Orchestrate**
- Airflow DAG scheduled hourly (`@hourly`)
- Slack + Email alerts on task failure  
  (Production-ready monitoring)

### ğŸ”¹ **Visualize**
- Metabase dashboards for interactive analytics.

---

# ğŸ§± 2. System Architecture

![Architecture Diagram](images/architecture.png)

```
                           ğŸŒ¤ OpenWeather API
                                   â”‚
                                   â–¼
                     ğŸŒ€ Apache Airflow (DAG Scheduler)
                       Extract â†’ Transform â†’ Load
                                   â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                           â”‚
           ğŸ“‚ Data Lake (Raw JSON)        ğŸ“ Clean Parquet Files
         data/raw/...                     data/clean/...
                     â”‚                           â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–¼
                         ğŸ˜ PostgreSQL Data Warehouse
                         (Star Schema: dim + fact)
                                   â”‚
                                   â–¼
                           ğŸ“Š Metabase Dashboard

```

---

# ğŸ“ 3. Project Folder Structure

```
weather_pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ weather_etl_dag.py
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”‚       â””â”€â”€ slack_alert.py
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract/
â”‚   â”œâ”€â”€ tranform/
â”‚   â””â”€â”€ load/
â”‚
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ clean/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

# ğŸ—„ 4. Data Warehouse Schema (Star Schema)

### â­ Dimensions
- `dim_city`  
- `dim_date`  
- `dim_weather_condition`

### â­ Fact Table
- `fact_weather`

### Key Metrics
- `temperature`, `feels_like`, `humidity`, `pressure`
- `wind_speed`, `feels_like_diff`
- `temp_category`, `weather_category`
- `wind_level`, `is_rain`
- `timestamp_utc`

---

# ğŸ³ 5. Running the Project with Docker

## Step 1 â€” Create `.env`

```
OPENWEATHER_API_KEY=your_api_key_here

# Email Alerts (SMTP)
SMTP_HOST=smtp.gmail.com
SMTP_STARTTLS=True
SMTP_SSL=False
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password
SMTP_PORT=587
SMTP_MAIL_FROM=your_email@gmail.com

# Slack Alerts
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/xxx/yyy/zzz
```

## Step 2 â€” Start the full platform

```
docker-compose up -d
```

### Exposed Services

| Service | Port | Description |
|--------|------|-------------|
| ğŸŒ€ Airflow Webserver | 8085 | DAG UI |
| ğŸ—„ PGAdmin | 8080 | Manage DWH |
| ğŸ˜ PostgreSQL DWH | 5432 | Data Warehouse |
| ğŸ“Š Metabase | 3000 | BI Dashboard |

---

# ğŸŒ€ 6. Airflow DAG Structure

The ETL workflow contains 3 tasks:

1. `extract_task`  
2. `tranform_task`  
3. `load_task`  

Scheduled hourly:

```python
schedule_interval="@hourly"
```

File: `airflow/dags/weather_etl_dag.py`

---

# ğŸ”” 7. Alerting & Monitoring

### ğŸ’¬ Slack Notifications
Sent when any task fails via incoming webhook.

### ğŸ“§ Email Alerts
Configured through Gmail App Password.

### ğŸ“ Enhanced Logging
- number of records processed  
- file paths  
- duration  
- task lifecycle events  

---

# ğŸ“Š 8. Metabase BI Dashboard

Includes:

- Temperature trend per city  
- Humidity & pressure comparison  
- Weather category distribution  
- Temperature vs humidity scatter plot
  
---

# ğŸ 9. Results

This project demonstrates:

- A complete end-to-end Data Engineering pipeline  
- Real-time weather ingestion  
- Production-grade orchestration using Airflow  
- Dimensional DWH modeling  
- Monitoring & alerting (Slack + Email)  
- Dockerized modular architecture  
- Interactive BI dashboards  



